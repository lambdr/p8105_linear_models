---
title: "Cross Validation"
author: "Derek Lamb"
date: "`r Sys.Date()`"
output: github_document
---
### Load packages
```{r load packages and default options, message = FALSE}
# Load packages
library(tidyverse)
library(modelr)
library(mgcv)

# Set default figure options
knitr::opts_chunk$set(
  fig.width = 6,
  out.width = "90%"
)

theme_set(theme_bw() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Fit on nonlinear data
```{r define data}
df_nonlin = 
  tibble(
    id = 1:100,
    x = runif(100, 0, 1),
    y = 1 - 10 * (x - 0.3) ^ 2 + rnorm(100, 0, 0.3)
    
  )
```

Look at the data set. It is not linear!
```{r scatterplot}
df_nonlin |> 
  ggplot(aes(x = x, y = y)) + 
  geom_point()
```

Do the train/test split.

```{r train/test by hand}
df_train = df_nonlin |> 
  sample_n(80) |> 
  arrange(id)

df_test = anti_join(
  df_nonlin,
  df_train,
  by = "id"
)
```

Visualize this split.
```{r plot test and train}
df_train |> 
  ggplot(aes(x = x, y = y)) +
  geom_point() +
  geom_point(data = df_test, color = "red")
```

Fit lm on `df_train`.
```{r fit lm}
linear_mod <- df_train |> 
  lm(y ~ x, data = _)

# Do this not quadratic terms??
smooth_mod = df_train |> 
  gam(y ~ s(x), data = _)

# Break this
jagged_mod = df_train |> 
  gam(y ~ s(x, k = 30), sp = 10e-6, data = _)
```

```{r look at fit linear}
df_train |> 
  add_predictions(linear_mod) |> 
  ggplot(aes(x = x, y = y)) +
  geom_point() + 
  geom_line(aes(y = pred), color = "red")
```

```{r look at fit smooth}
df_train |> 
  add_predictions(smooth_mod) |> 
  ggplot(aes(x = x, y = y)) +
  geom_point() + 
  geom_line(aes(y = pred), color = "red")
```

```{r look at fit jagged}
df_train |> 
  add_predictions(jagged_mod) |> 
  ggplot(aes(x = x, y = y)) +
  geom_point() + 
  geom_line(aes(y = pred), color = "red")
```

### Look at RMSE
```{r rmse of test vs train}
# Look at rmse of training data
rmse(linear_mod, df_train)
rmse(smooth_mod, df_train)
rmse(jagged_mod, df_train)

# Look at rmse of test data
rmse(linear_mod, df_test)
rmse(smooth_mod, df_test)
rmse(jagged_mod, df_test)

```

While the jagged fit does the best for the training data, it is worse than the smooth for the testing data. This is a sign that it is overfit.

## Use modelr for cv
Ok, doing stuff by hand is great, let's make it even greater.

```{r cv in modler}
df_cv = df_nonlin |> 
  crossv_mc(n = 100) |> 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  )

df_cv |> pull(train) |> nth(2)
# can convert back using as_tibble()
```

### Fit lots of models
```{r fit each model}
cv_results = df_cv |> 
  mutate(
    linear_fit = map(train, \(df) lm(y ~ x, data = df)),
    smooth_fit = map(train, \(df) gam(y ~ s(x), data = df)),
    jagged_fit = map(train, \(df) gam(y ~ s(x, k = 30), sp = 10e-6, data = df))
    ) |> 
  mutate(
    rmse_linear = map2_dbl(linear_fit, test, \(mod, df) rmse(mod, df)),
    rmse_smooth = map2_dbl(smooth_fit, test, \(mod, df) rmse(mod, df)),
    rmse_jagged = map2_dbl(jagged_fit, test, \(mod, df) rmse(mod, df))
    ) 
```

Now we can visualize the RMSE for each.

```{r}
cv_results |> 
  select(starts_with("rmse")) |> 
  pivot_longer(
    everything(),
    names_to = "model",
    values_to = "rmse",
    names_prefix = "rmse_"
  ) |> 
  ggplot(aes(y = rmse, group = model, color = model)) +
  geom_boxplot()
  
```

